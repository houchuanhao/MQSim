{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-06T13:30:38.910273Z",
     "start_time": "2024-04-06T13:30:35.698440Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression  # 假设使用线性回归作为预测模型\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from script.temp.tools.data_utils import data_utils\n",
    "data = data_utils('config.xlsx','result.xlsx')"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T13:30:40.917364Z",
     "start_time": "2024-04-06T13:30:40.853286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data.preprocess()\n",
    "#print(len(data.df_result))\n",
    "data.df_result = data.drop(['IOPS_Read', 'IOPS_Write', 'Bandwidth_Read', 'Bandwidth_Write'],0)\n",
    "#print(len(data.df_result))\n",
    "data.targetLst = ['IOPS_Read']\n",
    "X,y = data.dorp_and_encode()"
   ],
   "id": "3a5121b113a15d8a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1336\n",
      "1326\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T14:53:29.291341Z",
     "start_time": "2024-04-06T14:53:29.238822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# 准备数据\n",
    "# 假设X和y已经是DataFrame类型\n",
    "# 这里使用pandas的read_csv函数来示范，您可以替换成自己的数据加载方式\n",
    "# data = pd.read_csv('your_data.csv')\n",
    "# X = data.drop(columns=['target_column'])\n",
    "# y = data['target_column']\n",
    "\n",
    "# 示例数据\n",
    "X,y = data.dorp_and_encode()\n",
    "scaler = MinMaxScaler()\n",
    "X = X.values.astype(float)\n",
    "y = y.values.astype(float)\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_scaled = y.reshape(-1, 1)/10000\n",
    "\n",
    "# 划分数据集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# 转换为PyTorch张量\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)"
   ],
   "id": "b1da8783adc6aebf",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T14:54:09.571328Z",
     "start_time": "2024-04-06T14:53:55.939525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from script.temp.tools.data_utils import MLP, MPERegressionLoss\n",
    "\n",
    "# 参数设置\n",
    "input_size = X.shape[1]\n",
    "output_size = 1\n",
    "\n",
    "# 初始化模型\n",
    "model = MLP([input_size, 30,15,10,5, output_size])\n",
    "\n",
    "# 定义损失函数\n",
    "criterion = MPERegressionLoss()\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 定义数据加载器\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 300\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch+1) % 2 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# 评估模型\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor)\n",
    "    test_loss = criterion(y_pred.squeeze(), y_test_tensor)\n",
    "    print(f'Test Loss: {test_loss.item():.4f}')"
   ],
   "id": "1a58181c2bda9ca8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/300], Loss: 4.1589\n",
      "Epoch [4/300], Loss: 0.9681\n",
      "Epoch [6/300], Loss: 0.8282\n",
      "Epoch [8/300], Loss: 0.8801\n",
      "Epoch [10/300], Loss: 3.7910\n",
      "Epoch [12/300], Loss: 0.7825\n",
      "Epoch [14/300], Loss: 1.0843\n",
      "Epoch [16/300], Loss: 0.9743\n",
      "Epoch [18/300], Loss: 0.9043\n",
      "Epoch [20/300], Loss: 0.9360\n",
      "Epoch [22/300], Loss: 0.9409\n",
      "Epoch [24/300], Loss: 0.5265\n",
      "Epoch [26/300], Loss: 0.9880\n",
      "Epoch [28/300], Loss: 0.9889\n",
      "Epoch [30/300], Loss: 0.9916\n",
      "Epoch [32/300], Loss: 0.8727\n",
      "Epoch [34/300], Loss: 0.8798\n",
      "Epoch [36/300], Loss: 0.7657\n",
      "Epoch [38/300], Loss: 0.9827\n",
      "Epoch [40/300], Loss: 0.9011\n",
      "Epoch [42/300], Loss: 0.9256\n",
      "Epoch [44/300], Loss: 0.5845\n",
      "Epoch [46/300], Loss: 0.9269\n",
      "Epoch [48/300], Loss: 0.9208\n",
      "Epoch [50/300], Loss: 0.9034\n",
      "Epoch [52/300], Loss: 0.9841\n",
      "Epoch [54/300], Loss: 0.8874\n",
      "Epoch [56/300], Loss: 0.8915\n",
      "Epoch [58/300], Loss: 1.7341\n",
      "Epoch [60/300], Loss: 0.8968\n",
      "Epoch [62/300], Loss: 0.9607\n",
      "Epoch [64/300], Loss: 0.7350\n",
      "Epoch [66/300], Loss: 1.0080\n",
      "Epoch [68/300], Loss: 1.1479\n",
      "Epoch [70/300], Loss: 0.9633\n",
      "Epoch [72/300], Loss: 0.9729\n",
      "Epoch [74/300], Loss: 0.8659\n",
      "Epoch [76/300], Loss: 0.9700\n",
      "Epoch [78/300], Loss: 0.9412\n",
      "Epoch [80/300], Loss: 0.6156\n",
      "Epoch [82/300], Loss: 0.9045\n",
      "Epoch [84/300], Loss: 0.9251\n",
      "Epoch [86/300], Loss: 0.8377\n",
      "Epoch [88/300], Loss: 0.8473\n",
      "Epoch [90/300], Loss: 0.9784\n",
      "Epoch [92/300], Loss: 0.9392\n",
      "Epoch [94/300], Loss: 0.9552\n",
      "Epoch [96/300], Loss: 0.7589\n",
      "Epoch [98/300], Loss: 2.4337\n",
      "Epoch [100/300], Loss: 0.9601\n",
      "Epoch [102/300], Loss: 0.9536\n",
      "Epoch [104/300], Loss: 0.9201\n",
      "Epoch [106/300], Loss: 0.9209\n",
      "Epoch [108/300], Loss: 0.9674\n",
      "Epoch [110/300], Loss: 2.0746\n",
      "Epoch [112/300], Loss: 0.8799\n",
      "Epoch [114/300], Loss: 0.9369\n",
      "Epoch [116/300], Loss: 0.7956\n",
      "Epoch [118/300], Loss: 0.9254\n",
      "Epoch [120/300], Loss: 0.6950\n",
      "Epoch [122/300], Loss: 0.7453\n",
      "Epoch [124/300], Loss: 0.5318\n",
      "Epoch [126/300], Loss: 0.7678\n",
      "Epoch [128/300], Loss: 0.9789\n",
      "Epoch [130/300], Loss: 1.0008\n",
      "Epoch [132/300], Loss: 0.9166\n",
      "Epoch [134/300], Loss: 0.9259\n",
      "Epoch [136/300], Loss: 0.7682\n",
      "Epoch [138/300], Loss: 0.7957\n",
      "Epoch [140/300], Loss: 0.8822\n",
      "Epoch [142/300], Loss: 0.8497\n",
      "Epoch [144/300], Loss: 0.7396\n",
      "Epoch [146/300], Loss: 0.8577\n",
      "Epoch [148/300], Loss: 1.0119\n",
      "Epoch [150/300], Loss: 0.8885\n",
      "Epoch [152/300], Loss: 0.8240\n",
      "Epoch [154/300], Loss: 0.9105\n",
      "Epoch [156/300], Loss: 0.9554\n",
      "Epoch [158/300], Loss: 0.8222\n",
      "Epoch [160/300], Loss: 0.9719\n",
      "Epoch [162/300], Loss: 0.9387\n",
      "Epoch [164/300], Loss: 0.9955\n",
      "Epoch [166/300], Loss: 0.9853\n",
      "Epoch [168/300], Loss: 0.6850\n",
      "Epoch [170/300], Loss: 0.9846\n",
      "Epoch [172/300], Loss: 0.8846\n",
      "Epoch [174/300], Loss: 0.9536\n",
      "Epoch [176/300], Loss: 0.9968\n",
      "Epoch [178/300], Loss: 0.8815\n",
      "Epoch [180/300], Loss: 0.8218\n",
      "Epoch [182/300], Loss: 0.8837\n",
      "Epoch [184/300], Loss: 0.9719\n",
      "Epoch [186/300], Loss: 0.9448\n",
      "Epoch [188/300], Loss: 0.9277\n",
      "Epoch [190/300], Loss: 0.9772\n",
      "Epoch [192/300], Loss: 0.8955\n",
      "Epoch [194/300], Loss: 0.9723\n",
      "Epoch [196/300], Loss: 0.8325\n",
      "Epoch [198/300], Loss: 1.4189\n",
      "Epoch [200/300], Loss: 3.6782\n",
      "Epoch [202/300], Loss: 0.5018\n",
      "Epoch [204/300], Loss: 5.7953\n",
      "Epoch [206/300], Loss: 0.6649\n",
      "Epoch [208/300], Loss: 0.8538\n",
      "Epoch [210/300], Loss: 0.9309\n",
      "Epoch [212/300], Loss: 0.9263\n",
      "Epoch [214/300], Loss: 0.9532\n",
      "Epoch [216/300], Loss: 0.8872\n",
      "Epoch [218/300], Loss: 0.9820\n",
      "Epoch [220/300], Loss: 0.6836\n",
      "Epoch [222/300], Loss: 0.8270\n",
      "Epoch [224/300], Loss: 0.9628\n",
      "Epoch [226/300], Loss: 0.9205\n",
      "Epoch [228/300], Loss: 0.9537\n",
      "Epoch [230/300], Loss: 0.7964\n",
      "Epoch [232/300], Loss: 0.8925\n",
      "Epoch [234/300], Loss: 0.9924\n",
      "Epoch [236/300], Loss: 0.7071\n",
      "Epoch [238/300], Loss: 0.9818\n",
      "Epoch [240/300], Loss: 0.9189\n",
      "Epoch [242/300], Loss: 0.9802\n",
      "Epoch [244/300], Loss: 1.0445\n",
      "Epoch [246/300], Loss: 0.6479\n",
      "Epoch [248/300], Loss: 0.7333\n",
      "Epoch [250/300], Loss: 0.8952\n",
      "Epoch [252/300], Loss: 0.9198\n",
      "Epoch [254/300], Loss: 1.0021\n",
      "Epoch [256/300], Loss: 1.0113\n",
      "Epoch [258/300], Loss: 0.7885\n",
      "Epoch [260/300], Loss: 0.7801\n",
      "Epoch [262/300], Loss: 0.8451\n",
      "Epoch [264/300], Loss: 0.8667\n",
      "Epoch [266/300], Loss: 0.9296\n",
      "Epoch [268/300], Loss: 0.7252\n",
      "Epoch [270/300], Loss: 0.9417\n",
      "Epoch [272/300], Loss: 0.6661\n",
      "Epoch [274/300], Loss: 0.9005\n",
      "Epoch [276/300], Loss: 0.6977\n",
      "Epoch [278/300], Loss: 0.9653\n",
      "Epoch [280/300], Loss: 0.9971\n",
      "Epoch [282/300], Loss: 0.8155\n",
      "Epoch [284/300], Loss: 0.6881\n",
      "Epoch [286/300], Loss: 0.9666\n",
      "Epoch [288/300], Loss: 0.9683\n",
      "Epoch [290/300], Loss: 0.9583\n",
      "Epoch [292/300], Loss: 1.0049\n",
      "Epoch [294/300], Loss: 0.8360\n",
      "Epoch [296/300], Loss: 0.9467\n",
      "Epoch [298/300], Loss: 0.9130\n",
      "Epoch [300/300], Loss: 0.9039\n",
      "Test Loss: 1.1650\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T14:55:01.086583Z",
     "start_time": "2024-04-06T14:54:47.540097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 训练模型\n",
    "num_epochs = 300\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch+1) % 2 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# 评估模型\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor)\n",
    "    test_loss = criterion(y_pred.squeeze(), y_test_tensor)\n",
    "    print(f'Test Loss: {test_loss.item():.4f}')"
   ],
   "id": "ef9bb73897dcc8cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/300], Loss: 0.9660\n",
      "Epoch [4/300], Loss: 0.6450\n",
      "Epoch [6/300], Loss: 0.8070\n",
      "Epoch [8/300], Loss: 0.7939\n",
      "Epoch [10/300], Loss: 0.8704\n",
      "Epoch [12/300], Loss: 0.9081\n",
      "Epoch [14/300], Loss: 0.9453\n",
      "Epoch [16/300], Loss: 0.9167\n",
      "Epoch [18/300], Loss: 0.9697\n",
      "Epoch [20/300], Loss: 0.8751\n",
      "Epoch [22/300], Loss: 0.9846\n",
      "Epoch [24/300], Loss: 0.8110\n",
      "Epoch [26/300], Loss: 0.9959\n",
      "Epoch [28/300], Loss: 0.8336\n",
      "Epoch [30/300], Loss: 0.9659\n",
      "Epoch [32/300], Loss: 2.0484\n",
      "Epoch [34/300], Loss: 2.9594\n",
      "Epoch [36/300], Loss: 0.9776\n",
      "Epoch [38/300], Loss: 1.5702\n",
      "Epoch [40/300], Loss: 0.9896\n",
      "Epoch [42/300], Loss: 0.6164\n",
      "Epoch [44/300], Loss: 0.7025\n",
      "Epoch [46/300], Loss: 0.9812\n",
      "Epoch [48/300], Loss: 0.8321\n",
      "Epoch [50/300], Loss: 0.9905\n",
      "Epoch [52/300], Loss: 0.8311\n",
      "Epoch [54/300], Loss: 0.9989\n",
      "Epoch [56/300], Loss: 0.9765\n",
      "Epoch [58/300], Loss: 0.9280\n",
      "Epoch [60/300], Loss: 0.9112\n",
      "Epoch [62/300], Loss: 0.9859\n",
      "Epoch [64/300], Loss: 0.5171\n",
      "Epoch [66/300], Loss: 0.8566\n",
      "Epoch [68/300], Loss: 0.8847\n",
      "Epoch [70/300], Loss: 0.7696\n",
      "Epoch [72/300], Loss: 0.9125\n",
      "Epoch [74/300], Loss: 0.8464\n",
      "Epoch [76/300], Loss: 0.6239\n",
      "Epoch [78/300], Loss: 0.9988\n",
      "Epoch [80/300], Loss: 0.7747\n",
      "Epoch [82/300], Loss: 0.9448\n",
      "Epoch [84/300], Loss: 0.9664\n",
      "Epoch [86/300], Loss: 0.7541\n",
      "Epoch [88/300], Loss: 1.2229\n",
      "Epoch [90/300], Loss: 0.7701\n",
      "Epoch [92/300], Loss: 0.8898\n",
      "Epoch [94/300], Loss: 0.9418\n",
      "Epoch [96/300], Loss: 0.8837\n",
      "Epoch [98/300], Loss: 0.7645\n",
      "Epoch [100/300], Loss: 0.9491\n",
      "Epoch [102/300], Loss: 1.0112\n",
      "Epoch [104/300], Loss: 0.8512\n",
      "Epoch [106/300], Loss: 0.9372\n",
      "Epoch [108/300], Loss: 0.9670\n",
      "Epoch [110/300], Loss: 0.8831\n",
      "Epoch [112/300], Loss: 0.8051\n",
      "Epoch [114/300], Loss: 0.9503\n",
      "Epoch [116/300], Loss: 0.9682\n",
      "Epoch [118/300], Loss: 0.8913\n",
      "Epoch [120/300], Loss: 0.7728\n",
      "Epoch [122/300], Loss: 0.9058\n",
      "Epoch [124/300], Loss: 0.9637\n",
      "Epoch [126/300], Loss: 0.9820\n",
      "Epoch [128/300], Loss: 2.9632\n",
      "Epoch [130/300], Loss: 0.9907\n",
      "Epoch [132/300], Loss: 0.9296\n",
      "Epoch [134/300], Loss: 0.9703\n",
      "Epoch [136/300], Loss: 0.8640\n",
      "Epoch [138/300], Loss: 4.3385\n",
      "Epoch [140/300], Loss: 0.9288\n",
      "Epoch [142/300], Loss: 0.9684\n",
      "Epoch [144/300], Loss: 0.9726\n",
      "Epoch [146/300], Loss: 0.8795\n",
      "Epoch [148/300], Loss: 0.8457\n",
      "Epoch [150/300], Loss: 0.9924\n",
      "Epoch [152/300], Loss: 0.8088\n",
      "Epoch [154/300], Loss: 0.9666\n",
      "Epoch [156/300], Loss: 0.9333\n",
      "Epoch [158/300], Loss: 0.9855\n",
      "Epoch [160/300], Loss: 0.6563\n",
      "Epoch [162/300], Loss: 0.7644\n",
      "Epoch [164/300], Loss: 0.9266\n",
      "Epoch [166/300], Loss: 0.4967\n",
      "Epoch [168/300], Loss: 0.8383\n",
      "Epoch [170/300], Loss: 0.9463\n",
      "Epoch [172/300], Loss: 0.9328\n",
      "Epoch [174/300], Loss: 0.9872\n",
      "Epoch [176/300], Loss: 1.0681\n",
      "Epoch [178/300], Loss: 0.9430\n",
      "Epoch [180/300], Loss: 0.8130\n",
      "Epoch [182/300], Loss: 0.7070\n",
      "Epoch [184/300], Loss: 1.4228\n",
      "Epoch [186/300], Loss: 0.9733\n",
      "Epoch [188/300], Loss: 0.6391\n",
      "Epoch [190/300], Loss: 0.9604\n",
      "Epoch [192/300], Loss: 0.7601\n",
      "Epoch [194/300], Loss: 1.0560\n",
      "Epoch [196/300], Loss: 0.7583\n",
      "Epoch [198/300], Loss: 0.9133\n",
      "Epoch [200/300], Loss: 0.8115\n",
      "Epoch [202/300], Loss: 0.7281\n",
      "Epoch [204/300], Loss: 0.6265\n",
      "Epoch [206/300], Loss: 0.9138\n",
      "Epoch [208/300], Loss: 0.8339\n",
      "Epoch [210/300], Loss: 0.9495\n",
      "Epoch [212/300], Loss: 0.8597\n",
      "Epoch [214/300], Loss: 4.3985\n",
      "Epoch [216/300], Loss: 0.9719\n",
      "Epoch [218/300], Loss: 0.7974\n",
      "Epoch [220/300], Loss: 0.9412\n",
      "Epoch [222/300], Loss: 0.6237\n",
      "Epoch [224/300], Loss: 0.9672\n",
      "Epoch [226/300], Loss: 0.9196\n",
      "Epoch [228/300], Loss: 0.8988\n",
      "Epoch [230/300], Loss: 0.9532\n",
      "Epoch [232/300], Loss: 0.9538\n",
      "Epoch [234/300], Loss: 0.7898\n",
      "Epoch [236/300], Loss: 0.7032\n",
      "Epoch [238/300], Loss: 0.8919\n",
      "Epoch [240/300], Loss: 0.8363\n",
      "Epoch [242/300], Loss: 0.8253\n",
      "Epoch [244/300], Loss: 0.8537\n",
      "Epoch [246/300], Loss: 0.9605\n",
      "Epoch [248/300], Loss: 0.9123\n",
      "Epoch [250/300], Loss: 1.3319\n",
      "Epoch [252/300], Loss: 0.6651\n",
      "Epoch [254/300], Loss: 0.8968\n",
      "Epoch [256/300], Loss: 0.9877\n",
      "Epoch [258/300], Loss: 1.2557\n",
      "Epoch [260/300], Loss: 0.9669\n",
      "Epoch [262/300], Loss: 0.8369\n",
      "Epoch [264/300], Loss: 0.8450\n",
      "Epoch [266/300], Loss: 1.0124\n",
      "Epoch [268/300], Loss: 0.9699\n",
      "Epoch [270/300], Loss: 0.9562\n",
      "Epoch [272/300], Loss: 0.5635\n",
      "Epoch [274/300], Loss: 0.9158\n",
      "Epoch [276/300], Loss: 0.8814\n",
      "Epoch [278/300], Loss: 0.9624\n",
      "Epoch [280/300], Loss: 0.9613\n",
      "Epoch [282/300], Loss: 0.7463\n",
      "Epoch [284/300], Loss: 0.9928\n",
      "Epoch [286/300], Loss: 0.9933\n",
      "Epoch [288/300], Loss: 0.9336\n",
      "Epoch [290/300], Loss: 0.9837\n",
      "Epoch [292/300], Loss: 0.8113\n",
      "Epoch [294/300], Loss: 0.9899\n",
      "Epoch [296/300], Loss: 0.4189\n",
      "Epoch [298/300], Loss: 0.9891\n",
      "Epoch [300/300], Loss: 0.9223\n",
      "Test Loss: 0.9706\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T14:48:30.009684Z",
     "start_time": "2024-04-06T14:48:29.969360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 计算均方误差（MSE）\n",
    "mse_loss = nn.MSELoss()\n",
    "mse = mse_loss(y_pred.squeeze(), y_test_tensor)\n",
    "\n",
    "# 计算平均百分比误差（MPE）\n",
    "mpe = torch.mean(torch.abs((y_test_tensor - y_pred) / y_test_tensor)) * 100\n",
    "\n",
    "# 打印结果\n",
    "print(f'MSE: {mse.item():.4f}')\n",
    "print(f'MPE: {mpe.item():.4f}')"
   ],
   "id": "e7cd3cf6723a09b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.0024\n",
      "MPE: 9351.1162\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T14:40:07.567164Z",
     "start_time": "2024-04-06T14:40:07.522940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "y_pred_original = scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "y_test_original = scaler.inverse_transform(y_test_tensor.reshape(-1, 1)).flatten()\n",
    "\n",
    "# 创建包含反向缩放后的预测值和目标值的DataFrame\n",
    "df_results = pd.DataFrame({'y_pred_original': y_pred_original, 'y_true_original': y_test_original})\n",
    "percentage_error = np.abs((y_pred_original - y_test_original) / y_test_original) * 100\n",
    "\n",
    "# 将测试损失存储到DataFrame中\n",
    "df_results['test_loss'] = percentage_error\n",
    "df_results"
   ],
   "id": "c4e98ffb80e777a0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     y_pred_original  y_true_original    test_loss\n",
       "0        2642.804683      1242.299989   112.734823\n",
       "1        2392.481435       331.899987   620.844088\n",
       "2        5039.910986      2828.900092    78.157970\n",
       "3        2808.820043       810.899996   246.383038\n",
       "4       79091.492294     72439.799594     9.182373\n",
       "..               ...              ...          ...\n",
       "261      2393.078470       213.300002  1021.930825\n",
       "262      4675.666944      5126.500043     8.794169\n",
       "263      4083.315967       974.600034   318.973510\n",
       "264     48840.390471     42097.699780    16.016767\n",
       "265      2660.861656      1299.599949   104.744672\n",
       "\n",
       "[266 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_pred_original</th>\n",
       "      <th>y_true_original</th>\n",
       "      <th>test_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2642.804683</td>\n",
       "      <td>1242.299989</td>\n",
       "      <td>112.734823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2392.481435</td>\n",
       "      <td>331.899987</td>\n",
       "      <td>620.844088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5039.910986</td>\n",
       "      <td>2828.900092</td>\n",
       "      <td>78.157970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2808.820043</td>\n",
       "      <td>810.899996</td>\n",
       "      <td>246.383038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79091.492294</td>\n",
       "      <td>72439.799594</td>\n",
       "      <td>9.182373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>2393.078470</td>\n",
       "      <td>213.300002</td>\n",
       "      <td>1021.930825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>4675.666944</td>\n",
       "      <td>5126.500043</td>\n",
       "      <td>8.794169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>4083.315967</td>\n",
       "      <td>974.600034</td>\n",
       "      <td>318.973510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>48840.390471</td>\n",
       "      <td>42097.699780</td>\n",
       "      <td>16.016767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>2660.861656</td>\n",
       "      <td>1299.599949</td>\n",
       "      <td>104.744672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>266 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T14:14:43.045615Z",
     "start_time": "2024-04-06T14:14:43.011026Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "561d09155a20f5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([231.7])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T14:14:44.646290Z",
     "start_time": "2024-04-06T14:14:44.606732Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6f85d2e00d508982",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "231.7"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
