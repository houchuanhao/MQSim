{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8314c615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "/opt/homebrew/anaconda3/bin/python\n",
      "no scenario collection/workspace_ssd0_workload0/1854/workload_scenario_1.xml\n",
      "no scenario collection/workspace_ssd0_workload0/425/workload_scenario_1.xml\n",
      "no scenario collection/workspace_ssd0_workload0/866/workload_scenario_1.xml\n",
      "no scenario collection/workspace_ssd0_workload0/1994/workload_scenario_1.xml\n",
      "no scenario collection/workspace_ssd0_workload0/1702/workload_scenario_1.xml\n",
      "no scenario collection/workspace_ssd0_workload0/428/workload_scenario_1.xml\n",
      "no scenario collection/workspace_ssd0_workload0/3472/workload_scenario_1.xml\n",
      "no scenario collection/workspace_ssd0_workload0/1796/workload_scenario_1.xml\n",
      "no scenario collection/workspace_ssd0_workload0/2148/workload_scenario_1.xml\n",
      "no scenario collection/workspace_ssd0_workload0/2918/workload_scenario_1.xml\n",
      "no scenario collection/workspace_ssd0_workload0/3981/workload_scenario_1.xml\n",
      "no scenario collection/workspace_ssd0_workload0/2456/workload_scenario_1.xml\n",
      "no scenario collection/workspace_ssd0_workload0/1482/workload_scenario_1.xml\n",
      "collection:  3816\n",
      "total : 3830\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tools\n",
    "from math import sqrt\n",
    "from script.tools import tools\n",
    "#from script.collection import Parameter as pmt\n",
    "import sys\n",
    "print(sys.executable)\n",
    "import net\n",
    "import os\n",
    "from script.tools import collection\n",
    "#result = collection.collection(\"collection/workspace1\")  #[iops,dic_ssd,dic_worload,f.path]\n",
    "#result = collection.collection(\"collection/workspace2\",result)\n",
    "result = collection.collection(\"collection/workspace_ssd0_workload0\")\n",
    "#result = result[:1000]\n",
    "plst = collection.getParameters(path=\"collection/config.xlsx\",id = \"0\")\n",
    "expect = ['Seed','key','default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "def test(x_,y_,m, mean = 0,var = 1):\n",
    "    x_ = (x_ - mean)/var\n",
    "    y_pre = m(x_)\n",
    "    y_e = abs(y_pre - y_)/y_\n",
    "    print(torch.sum(y_e)/len(y_))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "['Overprovisioning_Ratio', 'Flash_Channel_Count', 'Channel_Transfer_Rate', 'Chip_No_Per_Channel', 'Page_Read_Latency_LSB', 'Page_Read_Latency_MSB', 'Page_Program_Latency_LSB', 'Page_Program_Latency_MSB', 'Die_No_Per_Chip', 'Plane_No_Per_Die', 'Block_No_Per_Plane', 'Page_No_Per_Block', 'Channel_IDs', 'Chip_IDs', 'Die_IDs', 'Plane_IDs', 'Initial_Occupancy_Percentage', 'Working_Set_Percentage', 'Read_Percentage']\n",
      "torch.Size([2671, 19])\n"
     ]
    }
   ],
   "source": [
    "#print(result[0])\n",
    "from script.tools import collection\n",
    "split_index = int(len(result)*0.7)\n",
    "train_result = result[:split_index]\n",
    "test_result  = result[split_index:]\n",
    "x_train_lst, y_train_lst, keys = collection.result2lst(train_result,plst,expect)\n",
    "x_test_lst,  y_test_lst, keys = collection.result2lst(test_result,plst,expect)\n",
    "x_train_tensor = torch.tensor(  x_train_lst  , dtype=torch.float)\n",
    "y_train_tensor = torch.tensor(  y_train_lst  , dtype=torch.float)/10000\n",
    "x_test_tensor =  torch.tensor(  x_test_lst   , dtype=torch.float)\n",
    "y_test_tensor =  torch.tensor(  y_test_lst   , dtype=torch.float)/10000\n",
    "x_mean = x_train_tensor.mean(dim = 0)\n",
    "x_var = x_train_tensor.var(dim = 0)\n",
    "x_train_tensor_nor = (x_train_tensor -x_mean)/x_var\n",
    "# x用正则化后的数据，y除以10000，得到模型f(x-u)\n",
    "dataset = collection.CustomDataset(x_train_tensor_nor, y_train_tensor)\n",
    "batch_size = 5\n",
    "dataloader = collection.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "model = net.DeepNet(19,8,1,5)\n",
    "criterion = nn.MSELoss()\n",
    "print(len(keys))\n",
    "print(keys)\n",
    "print(x_train_tensor.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "def lasso(x,y):\n",
    "    x_np = x.detach().numpy()\n",
    "    y_np = y.detach().numpy().ravel()\n",
    "    param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10]}\n",
    "    lasso = Lasso()\n",
    "    grid_search = GridSearchCV(lasso, param_grid, cv=5)\n",
    "    grid_search.fit(x_np, y_np)\n",
    "    best_alpha = grid_search.best_params_['alpha']\n",
    "    lasso_best = Lasso(alpha=best_alpha)\n",
    "    lasso_best.fit(x_np, y_np)\n",
    "    nonzero_coeff_indices = [i for i, coeff in enumerate(lasso_best.coef_) if coeff != 0]\n",
    "    x_reduced_np = x_np[:, nonzero_coeff_indices]\n",
    "    x_reduced = torch.tensor(x_reduced_np, dtype=torch.float32)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/200], Loss: 0.068887\n",
      "test:\n",
      "tensor(6.0088, grad_fn=<DivBackward0>)\n",
      "R2\n",
      "Epoch [200/200], Loss: 0.048858\n",
      "test:\n",
      "tensor(15.3267, grad_fn=<DivBackward0>)\n",
      "R2\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "num_epochs = 200\n",
    "lambda_l2 = 2e-6\n",
    "loss_history = []\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    i = 0\n",
    "    for batch_x, batch_y in dataloader: # x_train_tensor_nor\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        l2_regularization = torch.tensor(0.)\n",
    "        for param in model.parameters():\n",
    "            l2_regularization += torch.norm(param, p=2)\n",
    "            loss = loss + l2_regularization * lambda_l2\n",
    "        i = i + 1\n",
    "        if i == -1:\n",
    "            print(\"i=------\"+str(i),\"\\n outputs: \",outputs,\"\\n y:\",batch_y)\n",
    "        total_loss += loss.item()\n",
    "        if False:\n",
    "            print(\"   \\n \\n\")\n",
    "            print(\" outputs: \", outputs)\n",
    "            print(\" batch_y\", batch_y)\n",
    "            print(\" loss \", loss)\n",
    "            print(\"   \\n \\n\")\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        average_loss = total_loss / len(dataloader)\n",
    "        #print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.6f}')\n",
    "        print(\"test:\")\n",
    "        test(x_test_tensor,y_test_tensor,model,x_mean,x_std) # test准确度"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.9702e+01,  6.7141e-01,  7.5537e-04, -4.7359e-01,  5.0434e-06,\n",
      "        -4.4459e-06,  5.2011e-08,  6.5974e-07, -6.7162e-01,  3.0209e-01,\n",
      "         7.6860e-04,  3.5376e-03,  9.5929e-02, -2.2362e-01, -5.2445e-01,\n",
      "         1.0061e+00,  2.6238e-02, -3.2114e-02, -9.4298e-03])\n",
      "train\n",
      "tensor(15.4857, grad_fn=<DivBackward0>)\n",
      "\n",
      " \n",
      " test:\n",
      "tensor(15.3267, grad_fn=<DivBackward0>)\n",
      "\n",
      " \n",
      "\n",
      "out: tensor([0.6269])\n",
      "tensor([0.8681], grad_fn=<SelectBackward0>)\n",
      "19\n",
      "<class 'list'>\n",
      "[2331.9]\n"
     ]
    }
   ],
   "source": [
    "print(x_train_tensor_nor[0])\n",
    "# 训练集\n",
    "print(\"train\")\n",
    "test(x_train_tensor,y_train_tensor,model,x_mean,x_std)\n",
    "print(\"\\n \\n test:\")\n",
    "test(x_test_tensor,y_test_tensor,model,x_mean,x_std) # test\n",
    "#print(x_train_tensor[0],model(x_train_tensor_nor[0]),y_train_tensor[0])\n",
    "print(\"\\n \\n\")\n",
    "if 1 :\n",
    "    id = 762\n",
    "    print(\"out:\",y_test_tensor[id])\n",
    "    print(model( (x_test_tensor-x_mean)/x_var )[id])\n",
    "print(len(keys))\n",
    "print(keys)\n",
    "print(type(x_train_lst[0]))\n",
    "print(y_train_lst[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2368.34544935 -2045.66728335  3242.3426917  ...  5606.53286947\n",
      "  5468.93304925  4450.27732164]\n",
      "权重（斜率系数）:\n",
      " [-4.17607650e+02  4.32997828e+01  1.04073694e-01  4.46306813e+01\n",
      " -2.40524106e-03 -2.40163519e-03 -3.31336743e-04 -3.22609545e-04\n",
      "  4.40879654e+01  1.03167687e+01 -5.13006518e-02 -1.80649423e-02\n",
      "  9.62653185e+02  9.55254700e+02  2.41831674e+01  1.82711512e+02\n",
      "  4.73006850e-02 -7.94264185e-01  2.78277420e+01]\n",
      "截距（Intercept）:\n",
      " 393.19889208686345\n",
      "mse: 7009074.059436485\n",
      "决定系数 (R²): 0.5489657530453222\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "# 假设 x_train_lst 和 y_train_lst 已经被定义并且格式正确\n",
    "x_train = x_train_lst\n",
    "y_train = [y[0] for y in y_train_lst]  # 将 y_train_lst 转换为一维列表\n",
    "\n",
    "# 创建线性回归模型\n",
    "model = LinearRegression()\n",
    "\n",
    "# 训练模型\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# 进行预测（以 x_train 为例）\n",
    "y_pred = model.predict(x_train)\n",
    "\n",
    "# 打印预测结果\n",
    "print(y_pred)\n",
    "weights = model.coef_\n",
    "\n",
    "# 打印权重\n",
    "print(\"权重（斜率系数）:\\n\", weights)\n",
    "\n",
    "# 如果你也想获取截距\n",
    "intercept = model.intercept_\n",
    "print(\"截距（Intercept）:\\n\", intercept)\n",
    "mse = mean_squared_error(y_train, y_pred)\n",
    "print(\"mse:\",mse)\n",
    "r2 = r2_score(y_train, y_pred)\n",
    "print(\"决定系数 (R²):\", r2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no scenario collection/workspace/28/workload_scenario_1.xml\n",
      "no scenario collection/workspace/30/workload_scenario_1.xml\n",
      "collection:  6\n",
      "total : 8\n"
     ]
    }
   ],
   "source": [
    "xlst,ylst,keys = collection.result2lst(result,plst,expect)\n",
    "test_result = collection.collection(\"collection/workspace\")\n",
    "x_test_lst,y_test_lst,keys = collection.result2lst(test_result,plst,expect)\n",
    "x_tensor = torch.tensor(xlst,dtype=torch.float)\n",
    "y_tensor = torch.tensor(ylst,dtype=torch.float)\n",
    "x_test_tensor =  torch.tensor(x_test_lst,dtype = torch.float)\n",
    "y_test_tensor =  torch.tensor(y_test_lst,dtype = torch.float)\n",
    "\n",
    "model = net.DeepNet(48,48,1,5)\n",
    "criterion = nn.MSELoss()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_0.0982.pth  exists\n"
     ]
    },
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = net.DeepNet(48,48,1,5)\n",
    "#device = torch.device(\"mps\")\n",
    "#device = torch.device(\"cpu\")\n",
    "#model.to(device)\n",
    "criterion = nn.MSELoss()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5618],\n",
      "        [0.1125],\n",
      "        [0.2550],\n",
      "        [0.0654],\n",
      "        [0.0771],\n",
      "        [0.3035],\n",
      "        [0.4796],\n",
      "        [0.3741],\n",
      "        [0.1920],\n",
      "        [0.1301],\n",
      "        [0.1324],\n",
      "        [0.4283],\n",
      "        [0.1418],\n",
      "        [0.2323],\n",
      "        [0.2001],\n",
      "        [0.0292],\n",
      "        [0.2910],\n",
      "        [0.5038],\n",
      "        [0.0850],\n",
      "        [0.6712],\n",
      "        [0.5066],\n",
      "        [1.9664],\n",
      "        [0.0620],\n",
      "        [0.1514],\n",
      "        [0.5173],\n",
      "        [0.3189],\n",
      "        [0.6853],\n",
      "        [0.4222],\n",
      "        [0.2690],\n",
      "        [0.2659],\n",
      "        [0.0697],\n",
      "        [0.4649],\n",
      "        [0.3005],\n",
      "        [0.7437],\n",
      "        [0.2272],\n",
      "        [0.3342],\n",
      "        [0.2523],\n",
      "        [0.8178],\n",
      "        [0.1258],\n",
      "        [0.1817],\n",
      "        [0.6557],\n",
      "        [0.3641],\n",
      "        [0.0528],\n",
      "        [0.6096],\n",
      "        [0.2518],\n",
      "        [0.9682],\n",
      "        [0.1117],\n",
      "        [0.3666],\n",
      "        [0.0466],\n",
      "        [0.1309],\n",
      "        [0.2234],\n",
      "        [0.1421],\n",
      "        [0.2673],\n",
      "        [0.4538],\n",
      "        [0.1944],\n",
      "        [1.2781],\n",
      "        [0.3076],\n",
      "        [0.7153],\n",
      "        [0.0420],\n",
      "        [0.2027],\n",
      "        [0.2537],\n",
      "        [0.9758],\n",
      "        [0.2486],\n",
      "        [0.3178],\n",
      "        [0.2401],\n",
      "        [0.0598],\n",
      "        [0.6184],\n",
      "        [0.5161],\n",
      "        [0.2356],\n",
      "        [0.1335],\n",
      "        [0.6246],\n",
      "        [0.0838],\n",
      "        [0.2319],\n",
      "        [0.7669],\n",
      "        [0.0839],\n",
      "        [0.3304],\n",
      "        [1.2414],\n",
      "        [0.3496],\n",
      "        [0.4063],\n",
      "        [0.5419],\n",
      "        [0.2997],\n",
      "        [0.0806],\n",
      "        [0.4287],\n",
      "        [0.1585],\n",
      "        [0.7306],\n",
      "        [0.2225],\n",
      "        [0.1491],\n",
      "        [0.7924],\n",
      "        [0.3683],\n",
      "        [0.1112],\n",
      "        [0.0657],\n",
      "        [0.0520],\n",
      "        [0.0256],\n",
      "        [0.8761],\n",
      "        [0.4492],\n",
      "        [0.1601],\n",
      "        [0.1960],\n",
      "        [0.6194],\n",
      "        [0.1043],\n",
      "        [0.0837],\n",
      "        [0.2019],\n",
      "        [0.7112],\n",
      "        [0.0934],\n",
      "        [0.5888],\n",
      "        [0.5293],\n",
      "        [0.5089],\n",
      "        [0.2327],\n",
      "        [1.1285],\n",
      "        [0.4948],\n",
      "        [0.5091],\n",
      "        [0.4320],\n",
      "        [0.5338],\n",
      "        [0.5323],\n",
      "        [0.1104],\n",
      "        [0.2479],\n",
      "        [1.2498],\n",
      "        [0.1079],\n",
      "        [0.4749],\n",
      "        [1.1478],\n",
      "        [0.0773],\n",
      "        [0.2613],\n",
      "        [1.0411],\n",
      "        [0.1711],\n",
      "        [0.1508],\n",
      "        [0.6047],\n",
      "        [0.4024],\n",
      "        [0.6040],\n",
      "        [2.4512],\n",
      "        [0.2162],\n",
      "        [0.5146],\n",
      "        [0.2058],\n",
      "        [0.4329],\n",
      "        [0.0978],\n",
      "        [1.0419],\n",
      "        [0.1214],\n",
      "        [1.6107],\n",
      "        [0.0842],\n",
      "        [0.0902],\n",
      "        [0.3527],\n",
      "        [0.0966],\n",
      "        [0.2980],\n",
      "        [0.2612],\n",
      "        [0.2315],\n",
      "        [0.5009],\n",
      "        [0.2540],\n",
      "        [0.2212],\n",
      "        [0.0977],\n",
      "        [0.3764],\n",
      "        [0.0531],\n",
      "        [0.4523],\n",
      "        [0.0760],\n",
      "        [0.0765],\n",
      "        [0.6515],\n",
      "        [0.1373],\n",
      "        [0.2638],\n",
      "        [0.0408],\n",
      "        [0.0337],\n",
      "        [0.2547],\n",
      "        [0.4136],\n",
      "        [0.0703],\n",
      "        [0.1915],\n",
      "        [0.2797],\n",
      "        [0.3012],\n",
      "        [0.4590],\n",
      "        [0.5452],\n",
      "        [0.1499],\n",
      "        [0.2010],\n",
      "        [0.2694],\n",
      "        [0.6529],\n",
      "        [0.2241],\n",
      "        [0.8051],\n",
      "        [0.4607],\n",
      "        [0.4718],\n",
      "        [0.3640],\n",
      "        [0.0840],\n",
      "        [0.2983],\n",
      "        [0.3506],\n",
      "        [0.1237],\n",
      "        [0.1052],\n",
      "        [0.1076],\n",
      "        [0.0841],\n",
      "        [0.3029],\n",
      "        [0.3479],\n",
      "        [0.1864],\n",
      "        [0.1297],\n",
      "        [0.4385],\n",
      "        [0.0925],\n",
      "        [0.2294],\n",
      "        [0.4175],\n",
      "        [0.3856],\n",
      "        [0.3947],\n",
      "        [0.1052],\n",
      "        [0.3791],\n",
      "        [0.1012],\n",
      "        [0.0375],\n",
      "        [0.3240],\n",
      "        [1.0525],\n",
      "        [1.1783],\n",
      "        [0.4478],\n",
      "        [0.0490]], grad_fn=<AddmmBackward0>)\n",
      "test\n",
      "tensor(0.0775, grad_fn=<DivBackward0>)\n",
      "train:\n",
      "tensor(0.0695, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "# 自定义数据集类\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "# dataset = CustomDataset(x_tensor, y_tensor)\n",
    "y_modified = y_tensor/y_mean\n",
    "dataset = CustomDataset(normalized_x, y_modified)\n",
    "batch_size = 5\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 0.000246\n",
      "Epoch [20/1000], Loss: 0.000245\n",
      "Epoch [30/1000], Loss: 0.000244\n",
      "Epoch [40/1000], Loss: 0.000242\n",
      "Epoch [50/1000], Loss: 0.000242\n",
      "Epoch [60/1000], Loss: 0.000240\n",
      "Epoch [70/1000], Loss: 0.000240\n",
      "Epoch [80/1000], Loss: 0.000239\n",
      "Epoch [90/1000], Loss: 0.000237\n",
      "Epoch [100/1000], Loss: 0.000237\n",
      "Epoch [110/1000], Loss: 0.000236\n",
      "Epoch [120/1000], Loss: 0.000235\n",
      "Epoch [130/1000], Loss: 0.000235\n",
      "Epoch [140/1000], Loss: 0.000234\n",
      "Epoch [150/1000], Loss: 0.000233\n",
      "Epoch [160/1000], Loss: 0.000232\n",
      "Epoch [170/1000], Loss: 0.000231\n",
      "Epoch [180/1000], Loss: 0.000229\n",
      "Epoch [190/1000], Loss: 0.000229\n",
      "Epoch [200/1000], Loss: 0.000228\n",
      "Epoch [210/1000], Loss: 0.000228\n",
      "Epoch [220/1000], Loss: 0.000226\n",
      "Epoch [230/1000], Loss: 0.000225\n",
      "Epoch [240/1000], Loss: 0.000225\n",
      "Epoch [250/1000], Loss: 0.000223\n",
      "Epoch [260/1000], Loss: 0.000223\n",
      "Epoch [270/1000], Loss: 0.000221\n",
      "Epoch [280/1000], Loss: 0.000220\n",
      "Epoch [290/1000], Loss: 0.000220\n",
      "Epoch [300/1000], Loss: 0.000219\n",
      "Epoch [310/1000], Loss: 0.000218\n",
      "Epoch [320/1000], Loss: 0.000217\n",
      "Epoch [330/1000], Loss: 0.000216\n",
      "Epoch [340/1000], Loss: 0.000215\n",
      "Epoch [350/1000], Loss: 0.000214\n",
      "Epoch [360/1000], Loss: 0.000214\n",
      "Epoch [370/1000], Loss: 0.000213\n",
      "Epoch [380/1000], Loss: 0.000211\n",
      "Epoch [390/1000], Loss: 0.000211\n",
      "Epoch [400/1000], Loss: 0.000210\n",
      "Epoch [410/1000], Loss: 0.000209\n",
      "Epoch [420/1000], Loss: 0.000209\n",
      "Epoch [430/1000], Loss: 0.000207\n",
      "Epoch [440/1000], Loss: 0.000207\n",
      "Epoch [450/1000], Loss: 0.000206\n",
      "Epoch [460/1000], Loss: 0.000206\n",
      "Epoch [470/1000], Loss: 0.000204\n",
      "Epoch [480/1000], Loss: 0.000204\n",
      "Epoch [490/1000], Loss: 0.000202\n",
      "Epoch [500/1000], Loss: 0.000201\n",
      "Epoch [510/1000], Loss: 0.000201\n",
      "Epoch [520/1000], Loss: 0.000200\n",
      "Epoch [530/1000], Loss: 0.000199\n",
      "Epoch [540/1000], Loss: 0.000199\n",
      "Epoch [550/1000], Loss: 0.000198\n",
      "Epoch [560/1000], Loss: 0.000197\n",
      "Epoch [570/1000], Loss: 0.000196\n",
      "Epoch [580/1000], Loss: 0.000195\n",
      "Epoch [590/1000], Loss: 0.000195\n",
      "Epoch [600/1000], Loss: 0.000194\n",
      "Epoch [610/1000], Loss: 0.000193\n",
      "Epoch [620/1000], Loss: 0.000192\n",
      "Epoch [630/1000], Loss: 0.000192\n",
      "Epoch [640/1000], Loss: 0.000191\n",
      "Epoch [650/1000], Loss: 0.000190\n",
      "Epoch [660/1000], Loss: 0.000190\n",
      "Epoch [670/1000], Loss: 0.000189\n",
      "Epoch [680/1000], Loss: 0.000189\n",
      "Epoch [690/1000], Loss: 0.000187\n",
      "Epoch [700/1000], Loss: 0.000187\n",
      "Epoch [710/1000], Loss: 0.000186\n",
      "Epoch [720/1000], Loss: 0.000185\n",
      "Epoch [730/1000], Loss: 0.000184\n",
      "Epoch [740/1000], Loss: 0.000183\n",
      "Epoch [750/1000], Loss: 0.000183\n",
      "Epoch [760/1000], Loss: 0.000182\n",
      "Epoch [770/1000], Loss: 0.000181\n",
      "Epoch [780/1000], Loss: 0.000181\n",
      "Epoch [790/1000], Loss: 0.000180\n",
      "Epoch [800/1000], Loss: 0.000180\n",
      "Epoch [810/1000], Loss: 0.000179\n",
      "Epoch [820/1000], Loss: 0.000178\n",
      "Epoch [830/1000], Loss: 0.000178\n",
      "Epoch [840/1000], Loss: 0.000177\n",
      "Epoch [850/1000], Loss: 0.000176\n",
      "Epoch [860/1000], Loss: 0.000176\n",
      "Epoch [870/1000], Loss: 0.000175\n",
      "Epoch [880/1000], Loss: 0.000174\n",
      "Epoch [890/1000], Loss: 0.000173\n",
      "Epoch [900/1000], Loss: 0.000174\n",
      "Epoch [910/1000], Loss: 0.000172\n",
      "Epoch [920/1000], Loss: 0.000172\n",
      "Epoch [930/1000], Loss: 0.000171\n",
      "Epoch [940/1000], Loss: 0.000170\n",
      "Epoch [950/1000], Loss: 0.000170\n",
      "Epoch [960/1000], Loss: 0.000169\n",
      "Epoch [970/1000], Loss: 0.000168\n",
      "Epoch [980/1000], Loss: 0.000168\n",
      "Epoch [990/1000], Loss: 0.000167\n",
      "Epoch [1000/1000], Loss: 0.000166\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "num_epochs = 1000\n",
    "\n",
    "loss_history = []\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    i = 0\n",
    "    for batch_x, batch_y in dataloader:\n",
    "        #print(\"hhhh \")\n",
    "        #print(batch_x,batch_x)\n",
    "        #i = i +1\n",
    "        #print(batch_x,\"\\n\",batch_y)\n",
    "        #print(\"zzzzzzz\")\n",
    "        # 前向传播\n",
    "\n",
    "        outputs = model(batch_x)\n",
    "        #print(\"batch_x\",batch_x[0],\"\\n outputs: \",outputs[0])\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        i = i +1\n",
    "        if i == -1:\n",
    "            print(\"i=------\"+str(i),\"\\n outputs: \",outputs,\"\\n y:\",batch_y)\n",
    "        total_loss += loss.item()\n",
    "        if False:\n",
    "            print(\"   \\n \\n\")\n",
    "            print(\" outputs: \", outputs)\n",
    "            print(\" batch_y\", batch_y)\n",
    "            print(\" loss \", loss)\n",
    "\n",
    "            print(\"   \\n \\n\")\n",
    "        #print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "        #print(\"loss:\",loss.item(),\"batch_x: \",batch_x,\" outputs \",outputs,\" batch_y: \",batch_y)\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    #loss_history.append(total_loss)\n",
    "    # 打印训练信息\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        average_loss = total_loss / len(dataloader)\n",
    "        #print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.6f}')\n",
    "\n",
    "def test(x_,y_,m,rate_y, mean_x = 0,std_x = 1):\n",
    "    x_ = (x_ - mean_x)/std_x\n",
    "    y_pre = m(x_)*rate_y\n",
    "    y_e = abs(y_pre - y_)/y_\n",
    "    print(torch.sum(y_e)/len(y_))\n",
    "# 训练集\n",
    "test(x_tensor,y_tensor,model,y_mean,x_mean,x_std)\n",
    "\n",
    "print(\"\\n \\n test:\")\n",
    "test(x_test_tensor,y_test_tensor,model,y_mean,x_std)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2114, grad_fn=<DivBackward0>)\n",
      "tensor(0.2114, grad_fn=<DivBackward0>)\n",
      "\n",
      " \n",
      " test:\n",
      "tensor(1.3245, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "y_predict_train = model(normalized_x)\n",
    "y_err = (y_predict_train - y_modified)/y_modified\n",
    "print(torch.sum(abs(y_err))/len(y_predict))\n",
    "\n",
    "def test(x_,y_,m,rate_y, mean_x = 0,std_x = 1):\n",
    "    x_ = (x_ - mean_x)/std_x\n",
    "    y_pre = m(x_)*rate_y\n",
    "    y_e = abs(y_pre - y_)/y_\n",
    "    print(torch.sum(y_e)/len(y_))\n",
    "# 训练集\n",
    "test(x_tensor,y_tensor,model,y_mean,x_mean,x_std)\n",
    "\n",
    "print(\"\\n \\n test:\")\n",
    "test(x_test_tensor,y_test_tensor,model,y_mean,x_std)\n",
    "#print(len(x_tensor))\n",
    "# 测试集\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collection:  1070\n"
     ]
    }
   ],
   "source": [
    "result1 = []\n",
    "result1 = collection.collection(\"collection/workspace\",result1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%s\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "['PCIe_Lane_Bandwidth', 'PCIe_Lane_Count', 'SATA_Processing_Delay', 'Queue_Fetch_Size', 'Data_Cache_DRAM_Row_Size', 'Data_Cache_DRAM_Data_Rate', 'Data_Cache_DRAM_Data_Busrt_Size', 'Data_Cache_DRAM_tRCD', 'Data_Cache_DRAM_tCL', 'Data_Cache_DRAM_tRP', 'Overprovisioning_Ratio', 'GC_Exec_Threshold', 'Preferred_suspend_erase_time_for_read', 'Preferred_suspend_erase_time_for_write', 'Preferred_suspend_write_time_for_read', 'Flash_Channel_Count', 'Flash_Channel_Width', 'Channel_Transfer_Rate', 'Chip_No_Per_Channel', 'Page_Read_Latency_LSB', 'Page_Read_Latency_CSB', 'Page_Read_Latency_MSB', 'Page_Program_Latency_LSB', 'Page_Program_Latency_CSB', 'Page_Program_Latency_MSB', 'Block_Erase_Latency', 'Block_PE_Cycles_Limit', 'Suspend_Erase_Time', 'Suspend_Program_Time', 'Die_No_Per_Chip', 'Plane_No_Per_Die', 'Block_No_Per_Plane', 'Page_No_Per_Block', 'Page_Capacity', 'Page_Metadat_Capacity', 'Channel_IDs', 'Chip_IDs', 'Die_IDs', 'Plane_IDs', 'Initial_Occupancy_Percentage', 'Working_Set_Percentage', 'Read_Percentage', 'Percentage_of_Hot_Region', 'Address_Alignment_Unit', 'Average_Request_Size', 'Variance_Request_Size', 'Average_No_of_Reqs_in_Queue', 'Intensity']\n",
      "lst2excel test.xlsx\n"
     ]
    }
   ],
   "source": [
    "#print(xlst[0])\n",
    "print(len(xlst[0]))\n",
    "tx = []\n",
    "tx.append(keys)\n",
    "i = 0\n",
    "for y_ in ylst:\n",
    "    x_ = xlst[i].copy()\n",
    "    x_.append(y_[0])\n",
    "    tx.append(x_)\n",
    "    i = i + 1\n",
    "print(tx[0])\n",
    "tools.lst2excel(tx,\"test.xlsx\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "Python版本: 3.9.13\n"
     ]
    }
   ],
   "source": [
    "print(len(keys))\n",
    "import platform\n",
    "\n",
    "print(\"Python版本:\", platform.python_version())"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
